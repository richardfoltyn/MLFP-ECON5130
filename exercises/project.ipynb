{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting house prices with linear models\n",
    "\n",
    "In this project, you will work with the Ames house data set which we already encountered in the lectures. Your task is to evaluate the following four linear models in terms of their performance when predicting house prices:\n",
    "\n",
    "1. Linear regression\n",
    "2. Ridge regression\n",
    "3. Lasso\n",
    "4. Elastic net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*General hints:*\n",
    "\n",
    "1. Clearly label all graphs (axes, title, legend if required).\n",
    "2. When asked to provide a specific answer (e.g., \"Report the number of non-zero coefficients...\") make sure the answer is clearly printed in the notebook.\n",
    "3. Whenever a computation involves random number generation, initialise the seed to `123` to get reproducible results. Specifically, for `scikit-learn` functions this requires passing `random_state=123` where applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "The data is stored in `data/ames_houses.csv` in the course [GitHub repository](https://github.com/richardfoltyn/MLFP-ECON5130) and can be downloaded using the link\n",
    "[https://raw.githubusercontent.com/richardfoltyn/MLFP-ECON5130/main/data/ames_houses.csv](https://raw.githubusercontent.com/richardfoltyn/MLFP-ECON5130/main/data/ames_houses.csv).\n",
    "\n",
    "To load the data, you need to specify the file path depending on your computing environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this path if the CSV file is in the same directory as the Jupyter notebook\n",
    "file = 'ames_houses.csv'\n",
    "\n",
    "# Use this path if you want to download the file directly from Github\n",
    "# file = 'https://raw.githubusercontent.com/richardfoltyn/MLFP-ECON5130/main/exercises/ames_houses.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the CSV file as a `pandas` `DataFrame` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   SalePrice         1460 non-null   float64\n",
      " 1   LotArea           1460 non-null   float64\n",
      " 2   Neighborhood      1460 non-null   object \n",
      " 3   BuildingType      1386 non-null   object \n",
      " 4   OverallQuality    1460 non-null   int64  \n",
      " 5   OverallCondition  1460 non-null   int64  \n",
      " 6   YearBuilt         1460 non-null   int64  \n",
      " 7   CentralAir        1460 non-null   object \n",
      " 8   LivingArea        1460 non-null   float64\n",
      " 9   Bathrooms         1460 non-null   int64  \n",
      " 10  Bedrooms          1460 non-null   int64  \n",
      " 11  Fireplaces        1460 non-null   int64  \n",
      " 12  HasGarage         1460 non-null   int64  \n",
      "dtypes: float64(3), int64(7), object(3)\n",
      "memory usage: 148.4+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file, sep=',')\n",
    "\n",
    "# Display columns in the data set\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The included variables are a simplified subset of the data available at [openml.org](https://www.openml.org/d/42165):\n",
    "\n",
    "- `SalePrice`: House price in US dollars (float)\n",
    "- `LotArea`: Size of the lot in m² (float)\n",
    "- `Neighborhood`: Name of the neighborhood (string)\n",
    "- `BuildingType`: Type of building (categorical stored as string)\n",
    "- `OverallQuality`: Rates the overall condition of the house from (1) \"very poor\" to (10) \"excellent\" (integer)\n",
    "- `OverallCondition`: Rates the overall material and finish of the house from (1) \"very poor\" to (10) \"excellent\" (integer)\n",
    "- `YearBuilt`: Original construction date (integer)\n",
    "- `CentralAir`: Central air conditioning: Yes/No (categorical string)\n",
    "- `LivingArea`: Above-ground living area in m² (float)\n",
    "- `Bathrooms`: Number of bathrooms (integer)\n",
    "- `Bedrooms`: Number of bedrooms (integer)\n",
    "- `Fireplaces`: Number of fireplaces (integer)\n",
    "- `HasGarage`: Indicator whether house has a garage (integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Apply the following steps to preprocess the data before estimation:\n",
    "\n",
    "1. Drop all rows which contain any missing values (`NaN`)\n",
    "\n",
    "    *Hint:* Use [`dropna()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dropna.html)\n",
    "    to remove rows with missing observations.\n",
    "    \n",
    "2. Trim outliers:\n",
    "    1. Compute the 1st and 99th percentiles of the variables \n",
    "        `SalePrice`, `LivingArea` and `LotArea`\n",
    "    2. Drop all observations in which any of these variables is below its\n",
    "        1st percentile or above its 99th percentile.\n",
    "\n",
    "    *Hint:* Use [`quantile()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html)\n",
    "    to compute the percentiles for the relevant variables. To convert percentiles to quantiles,\n",
    "    you need to divide by 100.\n",
    "\n",
    "3. Recode the string values in column `CentralAir` into numbers such that \n",
    "    `'N'` is mapped to 0 and `'Y'` is mapped to 1. Store this numerical variable\n",
    "    using the column name `HasCentralAir`.\n",
    "4. Recode the values in column `Fireplaces` and create the new variable `HasFireplace`\n",
    "    so that `HasFireplace = 1` whenever at least one fireplace is present and \n",
    "    `HasFireplace = 0` otherwise.\n",
    "5. Recode the string values in column `BuildingType` and create the new variable\n",
    "    `IsSingleFamily` which takes on the value 1 whenever a house is a \n",
    "    single-family home and 0 otherwise.\n",
    "6. Plot the kernel densities (or histograms) for the variables `SalePrice`, `LivingArea` and `LotArea`.\n",
    "    You will notice that all three variables have a \n",
    "    [right-skewed](https://en.wikipedia.org/wiki/Skewness) distribution.\n",
    "    \n",
    "    *Hint:* You can plot kernel densities using \n",
    "    [`DataFrame.plot.kde()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.kde.html)\n",
    "    and histograms with \n",
    "    [`DataFrame.plot.hist()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.hist.html)\n",
    "7. Convert the variables `SalePrice`, `LivingArea` and `LotArea` to (natural) logs and re-create\n",
    "    the kernel density or histogram plots for the logged variables.\n",
    "    Name the transformed columns `logSalePrice`, `logLivingArea` and `logLotArea`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation\n",
    "\n",
    "## Model specification\n",
    "\n",
    "You are now asked to estimate the following model of house prices\n",
    "as a function of house characteristics:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log(SalePrice_i) \n",
    "    &= \\alpha + f\\Bigl(\\log(LivingArea_i), ~\n",
    "        \\log(LotArea_i), ~OverallCondition_i, \\\\\n",
    "    & \\qquad \\qquad \\qquad  OverallQuality_i,~ \n",
    "    Bathrooms_i,~ Bedrooms_i\\Bigr) \\\\\n",
    "    &+ \\gamma_0 YearBuilt_i + \n",
    "    \\gamma_1 HasCentralAir_i + \n",
    "    \\gamma_2 HasFireplace_i + \n",
    "    \\gamma_3 IsSingleFamily_i + \n",
    "    \\epsilon_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $i$ indexes observations and $\\epsilon$ is an additive error term.\n",
    "The function $f(\\bullet)$ is a *polynomial of degree 3* in its\n",
    "arguments, i.e., it includes all terms and interactions of the given variables\n",
    "where the exponents sum to 3 or less:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(\\log(LivingArea_i), \\log(LotArea_i), \\dots)\n",
    "    &= \\beta_0 \\log(LivingArea_i) \n",
    "    + \\beta_1 \\log(LivingArea_i)^2 \\\\\n",
    "    &+ \\beta_2 \\log(LivingArea_i)^3 \n",
    "    + \\beta_3 \\log(LotArea_i) \\\\\n",
    "    &+ \\beta_4 \\log(LotArea_i)^2\n",
    "    + \\beta_5 \\log(LotArea_i)^3 \\\\\n",
    "    &+ \\beta_6 \\log(LivingArea_i)\\log(LotArea_i) \\\\\n",
    "    &+ \\beta_7 \\log(LivingArea_i)^2 \\log(LotArea_i) \\\\\n",
    "    &+ \\beta_8 \\log(LivingArea_i) \\log(LotArea_i)^2 \\\\\n",
    "    &+ \\cdots \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature matrix `X` which contains all polynomial interactions as well as the remaining non-interacted variables.\n",
    "\n",
    "*Hints:* \n",
    "\n",
    "- Use the \n",
    "[`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
    "transformation to create the polynomial terms and interactions from the columns\n",
    "    `logLivingArea`, `logLotArea`, \n",
    "    `OverallCondition`, `OverallQuality`, \n",
    "    `Bathrooms` and  `Bedrooms`.\n",
    "- Make sure that the generated polynomial does *not* contain a \n",
    "constant (\"bias\"). You should include the intercept when estimating a model instead.\n",
    "- You can use [`np.hstack()`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) to concatenate two matrices (the polynomials and the remaining covariates) along the column dimension.\n",
    "- The complete feature matrix `X` should contain a total of 87 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test sample split\n",
    "\n",
    "Split the data into a training and a test subset such that the training\n",
    "sample contains 70% of observations.\n",
    "\n",
    "*Hint:* \n",
    "\n",
    "- Use the function [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the sample.\n",
    "    Pass the argument `random_state=123` to get reproducible results.\n",
    "- Make sure to define the training and test samples only *once* so that they are identical for all estimators used below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Perform the following tasks:\n",
    "\n",
    "1. Comment on whether you need to standardise features before estimating a linear regression model. Does the linear regression model have any hyperparameters?\n",
    "2. Estimate the above model specification using a linear regression model on the training sub-set. \n",
    "3. Compute and report the mean squared error (MSE) on the test sample.\n",
    "4. Plot the prediction errors (on the $y$-axis) against the outcome variable on the test sample.\n",
    "\n",
    "*Hints:*\n",
    "\n",
    "- Use the [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class to estimate the model.\n",
    "- The mean squared error can be computed with [`mean_squared_error()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression\n",
    "\n",
    "Perform the following tasks:\n",
    "\n",
    "1. Does Ridge regression require feature standardisation? If so, don't forget to apply it before fitting the model.\n",
    "2. Use [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
    "    to determine the best regularisation strength $\\alpha$ on the training sub-sample.\n",
    "    You can use the MSE metric (the default) to find the optimal $\\alpha$.\n",
    "    Report the optimal $\\alpha$ and the corresponding MSE.\n",
    "3. Plot the MSE (averaged over folds on the training sub-sample) against the regularisation strength $\\alpha$ on the $x$-axis (use a log scale for the $x$-axis).\n",
    "4. Compute and report the MSE on the test sample.\n",
    "5. Plot the prediction errors (on the $y$-axis) against the outcome variable on the test sample.\n",
    "\n",
    "*Hints:* \n",
    "\n",
    "- Determine a suitable range for the grid of candidate $\\alpha$ and space them uniformly in logs. \n",
    "- Recall that the (negative!) best MSE is stored in the attribute `best_score_` after cross-validation is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso\n",
    "\n",
    "Perform the following tasks:\n",
    "\n",
    "1. Does Lasso require feature standardisation? If so, don't forget to apply it before fitting the model.\n",
    "2. Use [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
    "    to determine the best regularisation strength $\\alpha$ on the training sub-sample\n",
    "    using cross-validation with 5 folds.\n",
    "    You can use the MSE metric (the default) to find the optimal $\\alpha$.\n",
    "    Report the optimal $\\alpha$ and the corresponding MSE.\n",
    "3. Plot the MSE (averaged over folds on the training sub-sample) against the regularisation strength $\\alpha$ on the $x$-axis (use a log scale for the $x$-axis).\n",
    "4. Compute and report the MSE on the test sample for the model using the optimal $\\alpha$.\n",
    "5. Report the number of non-zero coefficients for the model using the optimal $\\alpha$.\n",
    "6. Plot the prediction errors (on the $y$-axis) against the outcome variable on the test sample.\n",
    "\n",
    "*Hints:* \n",
    "\n",
    "- Getting Lasso to converge may require some experimentation. The following settings should help: increase the max. number of iterations to `max_iter=100000` and use `selection='random'`. Set `random_state=123` to get reproducible results:\n",
    "\n",
    "    ```python\n",
    "    LassoCV(..., max_iter=100000, selection='random', random_state=123)\n",
    "    ```\n",
    "\n",
    "- After cross-validation is complete, the MSE for each value of $\\alpha$ and each fold are stored in the attribute `mse_path_` which is an array with shape `(N_ALPHA, N_FOLDS)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic net\n",
    "\n",
    "The elastic net is a linear model that applies both L1 and L2 regularisation, i.e., it's a generalisation of Ridge regression and Lasso. Its loss function is given by\n",
    "$$\n",
    "L(\\mu, \\mathbf{\\beta}) = \\frac{1}{2N}\n",
    "    \\underbrace{\\sum_{i=1}^N \\Bigl(\n",
    "    y_i - \\mu - \\mathbf{x}_i'\\mathbf{\\beta}\\Bigr)^2}_{\\text{Sum of squared errors}}\n",
    "    + \n",
    "    \\underbrace{\\alpha \\rho \\sum_{k=1}^K |\\beta_k|}_{\\text{L1 penalty}}    \n",
    "    + \n",
    "    \\underbrace{\\alpha (1-\\rho) \\sum_{k=1}^K\\beta_k^2}_{\\text{L2 penalty}}\n",
    "$$\n",
    "The additional parameter $\\rho$ is called the L1 ratio and determines the relative weight of the L1 vs L2 penalty terms (see also the `scikit-learn` \n",
    "[user guide](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net)).\n",
    "Compared to Ridge regression and Lasso, this model therefore includes two hyperparameters, $\\alpha$ and $\\rho$, both of which should be determined using cross-validation.\n",
    "It is easy to see that for the corner case of $\\rho = 1$, the elastic net corresponds to the Lasso, while for $\\rho = 0$ is corresponds to Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following tasks:\n",
    "\n",
    "1. Does the elastic net require feature standardisation?\n",
    "2. Use [`ElasticNetCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html)\n",
    "    to determine the best regularisation strength $\\alpha$ and L1 ratio $\\rho$ on the training sub-sample\n",
    "    using cross-validation with 5 folds.\n",
    "    You can use the MSE metric (the default) to find the optimal hyperparameter values.\n",
    "    Report the optimal $\\alpha$ and $\\rho$ and the corresponding MSE.\n",
    "3. Compute and report the MSE on the test sample for the model with optimal hyperparameters.\n",
    "4. Report the number of non-zero coefficients for the model with optimal hyperparameters.\n",
    "5. Plot the prediction errors (on the $y$-axis) against the outcome variable on the test sample.\n",
    "\n",
    "*Hints:* \n",
    "\n",
    "- Getting elastic net to converge may require some experimentation. The following settings should help: increase the max. number of iterations to `max_iter=100000` and use `selection='random'`. Set `random_state=123` to get reproducible results. \n",
    "\n",
    "    ```python\n",
    "    ElasticNetCV(..., max_iter=100000, selection='random', random_state=123)\n",
    "    ```\n",
    "\n",
    "- The grid for $\\alpha$ is determined in the same way as for `LassoCV`. For $\\rho$, use the argument `l1_ratio` to pass a grid of candidate L1 rations given by `[0.1, 0.5, 0.7, 0.9, 0.95, 0.99]`:\n",
    "\n",
    "    ```python\n",
    "    ElasticNetCV(..., l1_ratios=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99], ...)\n",
    "    ```\n",
    "\n",
    "- Use [`ElasticNet`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) to estimate the elastic net once you identified the optimal hyperparameters.\n",
    "    Make sure to pass the same values for `max_iter`, `selection` and `random_state` as you did earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare estimation results\n",
    "\n",
    "Create a table which contains the MSE computed on the test sample for all four models (using their optimal hyperparameters). Which model yields the lowest MSE? Comment on why you think this is the case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py3-default')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89692ee4bd8d7a7842eb7c7050f10ae8c4113955275de7625334d5364ea86119"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
